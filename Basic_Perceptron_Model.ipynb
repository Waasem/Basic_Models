{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_Perceptron_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPG04-Qwd8TL"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "%matplotlib inline \n",
        "# sets the backend of matplotlib to the 'inline' backend, with this backend, the output of plotting commands is displayed inline directly\n",
        "# below the code cell that produced it. The resulting plots will then also be stored in the notebook document.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEDVpLlUg9VG",
        "outputId": "bebf2ce3-45e4-4ff7-fadb-cfc3bac92c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "2fa244c15e4a4c3596d284112275c5bd",
            "b7a715229a1b4734b79537756d0b5e1c",
            "3b9863b8c6544556bd35991694bb03a6",
            "df6211bcc13b400a8b104d1085576541",
            "a6f1c39d0d6b49b78a6cca212792504d",
            "50c843c882bc4298a4f00e6873880766",
            "b6f5a41d28b4425885a195bda74814e5",
            "7206cdcc3abe4158aa04433500d496b6"
          ]
        }
      },
      "source": [
        "# Download CIFAR-10 dataset\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), # The output of torchvision datasets are PILImage images of range [0, 1]\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] # We transform them to Tensors of normalized range [-1, 1]\n",
        ")\n",
        "\n",
        "train_dataset = CIFAR10(root = './data', train = True, download=True, transform=ToTensor())\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
        "\n",
        "train_dataset_size = len(train_dataset)\n",
        "test_dataset_size = len(test_dataset)\n",
        "\n",
        "#classes = (plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "classes = train_dataset.classes\n",
        "num_classes = len(classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fa244c15e4a4c3596d284112275c5bd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZf5F-71eDWe"
      },
      "source": [
        "# Basic perceptron model\n",
        "class perceptron(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(perceptron, self).__init__()\n",
        "        self.fc = nn.Linear(1,1)            # y = xA.T + b, input = Nx1, output = Nx1\n",
        "        self.relu = torch.nn.ReLU()         # element-wise ReLU\n",
        "    def forward(self, x):\n",
        "        output = self.fc(x)\n",
        "        output = self.relu(x)\n",
        "        return output     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXeRFztkeIWT"
      },
      "source": [
        "class Feedforward(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Feedforward, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        \n",
        "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        hidden = self.fc1(x)\n",
        "        relu = self.relu(hidden)\n",
        "        \n",
        "        output = self.fc2(relu)\n",
        "        output = self.sigmoid(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g4O6cm7eLWj"
      },
      "source": [
        "# create random data points\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "def blob_label(y, label, loc): # assign labels\n",
        "    target = np.copy(y)\n",
        "    for l in loc:\n",
        "        target[y == l] = label\n",
        "    return target\n",
        "\n",
        "x_train, y_train = make_blobs(n_samples = 40, n_features = 2, cluster_std=1.5, shuffle = True)\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.FloatTensor(blob_label(y_train, 0, [0]))\n",
        "y_train = torch.FloatTensor(blob_label(y_train, 1, [1,2,3]))\n",
        "\n",
        "x_test, y_test = make_blobs(n_samples = 10, n_features = 2, cluster_std=1.5, shuffle = True)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.FloatTensor(blob_label(y_test, 0, [0]))\n",
        "y_test = torch.FloatTensor(blob_label(y_test, 1, [1,2,3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cqfBooWeN71"
      },
      "source": [
        "# model, criterion, optimizer\n",
        "model = Feedforward(2, 10)\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jt8DzrVeQXT",
        "outputId": "527cf9fa-0ab4-47b2-9fa0-e624dfa5c9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# evaluate model's performance before training\n",
        "\n",
        "model.eval()\n",
        "y_pred = model(x_test)\n",
        "before_train = criterion(y_pred.squeeze(), y_test)\n",
        "print(\"Test loss before training\", before_train.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss before training 0.7167186141014099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mg-tzGEeTNi",
        "outputId": "ad454ef1-6b96-4d5c-8e75-a551182fe650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# switch back to training mode\n",
        "\n",
        "model.train()\n",
        "epoch = 500\n",
        "\n",
        "for epoch in range(epoch):\n",
        "    optimizer.zero_grad() # set gradient to zero before start of backpropagation, pytorch\n",
        "                          # accumulates gradients from backward passes of previous epochs\n",
        "    \n",
        "    # Forward Pass\n",
        "    y_pred = model(x_train)\n",
        "    \n",
        "    # Compute Loss\n",
        "    loss = criterion(y_pred.squeeze(), y_train)\n",
        "    print('Epoch: {}, train loss: {}'.format(epoch, loss.item()))\n",
        "    \n",
        "    # Backward Pass\n",
        "    loss.backward() # compute gradients\n",
        "    optimizer.step() # update weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, train loss: 0.7925909757614136\n",
            "Epoch: 1, train loss: 0.7148780226707458\n",
            "Epoch: 2, train loss: 0.6482788324356079\n",
            "Epoch: 3, train loss: 0.591232180595398\n",
            "Epoch: 4, train loss: 0.5422729849815369\n",
            "Epoch: 5, train loss: 0.5001434087753296\n",
            "Epoch: 6, train loss: 0.4635399281978607\n",
            "Epoch: 7, train loss: 0.43163514137268066\n",
            "Epoch: 8, train loss: 0.40365567803382874\n",
            "Epoch: 9, train loss: 0.37900668382644653\n",
            "Epoch: 10, train loss: 0.3571118712425232\n",
            "Epoch: 11, train loss: 0.3375438451766968\n",
            "Epoch: 12, train loss: 0.31998375058174133\n",
            "Epoch: 13, train loss: 0.30416759848594666\n",
            "Epoch: 14, train loss: 0.2898276448249817\n",
            "Epoch: 15, train loss: 0.2767683267593384\n",
            "Epoch: 16, train loss: 0.2648358941078186\n",
            "Epoch: 17, train loss: 0.2538939416408539\n",
            "Epoch: 18, train loss: 0.24382655322551727\n",
            "Epoch: 19, train loss: 0.23453500866889954\n",
            "Epoch: 20, train loss: 0.22593474388122559\n",
            "Epoch: 21, train loss: 0.21795253455638885\n",
            "Epoch: 22, train loss: 0.2105253040790558\n",
            "Epoch: 23, train loss: 0.2035980522632599\n",
            "Epoch: 24, train loss: 0.19712263345718384\n",
            "Epoch: 25, train loss: 0.19105660915374756\n",
            "Epoch: 26, train loss: 0.18535597622394562\n",
            "Epoch: 27, train loss: 0.17999622225761414\n",
            "Epoch: 28, train loss: 0.17494800686836243\n",
            "Epoch: 29, train loss: 0.17018525302410126\n",
            "Epoch: 30, train loss: 0.16568708419799805\n",
            "Epoch: 31, train loss: 0.16143491864204407\n",
            "Epoch: 32, train loss: 0.15740437805652618\n",
            "Epoch: 33, train loss: 0.1535787433385849\n",
            "Epoch: 34, train loss: 0.14994296431541443\n",
            "Epoch: 35, train loss: 0.14648325741291046\n",
            "Epoch: 36, train loss: 0.14318722486495972\n",
            "Epoch: 37, train loss: 0.14004364609718323\n",
            "Epoch: 38, train loss: 0.13704216480255127\n",
            "Epoch: 39, train loss: 0.1341734081506729\n",
            "Epoch: 40, train loss: 0.13142888247966766\n",
            "Epoch: 41, train loss: 0.1288013607263565\n",
            "Epoch: 42, train loss: 0.12628275156021118\n",
            "Epoch: 43, train loss: 0.12386585772037506\n",
            "Epoch: 44, train loss: 0.12154374271631241\n",
            "Epoch: 45, train loss: 0.11931248754262924\n",
            "Epoch: 46, train loss: 0.11716679483652115\n",
            "Epoch: 47, train loss: 0.1151018962264061\n",
            "Epoch: 48, train loss: 0.11311458051204681\n",
            "Epoch: 49, train loss: 0.1112016811966896\n",
            "Epoch: 50, train loss: 0.1093568205833435\n",
            "Epoch: 51, train loss: 0.10757671296596527\n",
            "Epoch: 52, train loss: 0.10585863888263702\n",
            "Epoch: 53, train loss: 0.10419841855764389\n",
            "Epoch: 54, train loss: 0.10259322077035904\n",
            "Epoch: 55, train loss: 0.10104028135538101\n",
            "Epoch: 56, train loss: 0.09953714162111282\n",
            "Epoch: 57, train loss: 0.09808136522769928\n",
            "Epoch: 58, train loss: 0.09667093306779861\n",
            "Epoch: 59, train loss: 0.09530457854270935\n",
            "Epoch: 60, train loss: 0.09397851675748825\n",
            "Epoch: 61, train loss: 0.09269227087497711\n",
            "Epoch: 62, train loss: 0.09144429117441177\n",
            "Epoch: 63, train loss: 0.09023214876651764\n",
            "Epoch: 64, train loss: 0.08905428647994995\n",
            "Epoch: 65, train loss: 0.0879092589020729\n",
            "Epoch: 66, train loss: 0.08679571747779846\n",
            "Epoch: 67, train loss: 0.08571223169565201\n",
            "Epoch: 68, train loss: 0.08465622365474701\n",
            "Epoch: 69, train loss: 0.0836281031370163\n",
            "Epoch: 70, train loss: 0.08262675255537033\n",
            "Epoch: 71, train loss: 0.08165114372968674\n",
            "Epoch: 72, train loss: 0.08070026338100433\n",
            "Epoch: 73, train loss: 0.07977317273616791\n",
            "Epoch: 74, train loss: 0.07886900007724762\n",
            "Epoch: 75, train loss: 0.07798685133457184\n",
            "Epoch: 76, train loss: 0.07712593674659729\n",
            "Epoch: 77, train loss: 0.07628549635410309\n",
            "Epoch: 78, train loss: 0.07546480745077133\n",
            "Epoch: 79, train loss: 0.07466314733028412\n",
            "Epoch: 80, train loss: 0.07387985289096832\n",
            "Epoch: 81, train loss: 0.07311428338289261\n",
            "Epoch: 82, train loss: 0.07236585021018982\n",
            "Epoch: 83, train loss: 0.0716339573264122\n",
            "Epoch: 84, train loss: 0.07091804593801498\n",
            "Epoch: 85, train loss: 0.07021760940551758\n",
            "Epoch: 86, train loss: 0.0695321336388588\n",
            "Epoch: 87, train loss: 0.06886110454797745\n",
            "Epoch: 88, train loss: 0.0682041272521019\n",
            "Epoch: 89, train loss: 0.06756070256233215\n",
            "Epoch: 90, train loss: 0.06693042069673538\n",
            "Epoch: 91, train loss: 0.06631286442279816\n",
            "Epoch: 92, train loss: 0.06570764631032944\n",
            "Epoch: 93, train loss: 0.06511441618204117\n",
            "Epoch: 94, train loss: 0.0645328015089035\n",
            "Epoch: 95, train loss: 0.0639624297618866\n",
            "Epoch: 96, train loss: 0.06340300291776657\n",
            "Epoch: 97, train loss: 0.06285417079925537\n",
            "Epoch: 98, train loss: 0.06231565400958061\n",
            "Epoch: 99, train loss: 0.061787158250808716\n",
            "Epoch: 100, train loss: 0.06126837059855461\n",
            "Epoch: 101, train loss: 0.060759056359529495\n",
            "Epoch: 102, train loss: 0.06025891751050949\n",
            "Epoch: 103, train loss: 0.05976773053407669\n",
            "Epoch: 104, train loss: 0.059285156428813934\n",
            "Epoch: 105, train loss: 0.05881104990839958\n",
            "Epoch: 106, train loss: 0.058345187455415726\n",
            "Epoch: 107, train loss: 0.05788733810186386\n",
            "Epoch: 108, train loss: 0.057437293231487274\n",
            "Epoch: 109, train loss: 0.056994855403900146\n",
            "Epoch: 110, train loss: 0.05655981972813606\n",
            "Epoch: 111, train loss: 0.056131988763809204\n",
            "Epoch: 112, train loss: 0.055711209774017334\n",
            "Epoch: 113, train loss: 0.05529727786779404\n",
            "Epoch: 114, train loss: 0.05489002913236618\n",
            "Epoch: 115, train loss: 0.05448930338025093\n",
            "Epoch: 116, train loss: 0.05409493297338486\n",
            "Epoch: 117, train loss: 0.05370677635073662\n",
            "Epoch: 118, train loss: 0.05332464724779129\n",
            "Epoch: 119, train loss: 0.05294852331280708\n",
            "Epoch: 120, train loss: 0.052578408271074295\n",
            "Epoch: 121, train loss: 0.05221389979124069\n",
            "Epoch: 122, train loss: 0.05185488611459732\n",
            "Epoch: 123, train loss: 0.051501255482435226\n",
            "Epoch: 124, train loss: 0.05115284398198128\n",
            "Epoch: 125, train loss: 0.050809554755687714\n",
            "Epoch: 126, train loss: 0.05047129467129707\n",
            "Epoch: 127, train loss: 0.050137899816036224\n",
            "Epoch: 128, train loss: 0.04980931431055069\n",
            "Epoch: 129, train loss: 0.04948539286851883\n",
            "Epoch: 130, train loss: 0.04916603863239288\n",
            "Epoch: 131, train loss: 0.04885115846991539\n",
            "Epoch: 132, train loss: 0.04854068160057068\n",
            "Epoch: 133, train loss: 0.048234470188617706\n",
            "Epoch: 134, train loss: 0.047932468354701996\n",
            "Epoch: 135, train loss: 0.047634564340114594\n",
            "Epoch: 136, train loss: 0.047340668737888336\n",
            "Epoch: 137, train loss: 0.04705072566866875\n",
            "Epoch: 138, train loss: 0.04676462337374687\n",
            "Epoch: 139, train loss: 0.046482302248477936\n",
            "Epoch: 140, train loss: 0.046203676611185074\n",
            "Epoch: 141, train loss: 0.045928675681352615\n",
            "Epoch: 142, train loss: 0.04565722867846489\n",
            "Epoch: 143, train loss: 0.045389264822006226\n",
            "Epoch: 144, train loss: 0.04512470215559006\n",
            "Epoch: 145, train loss: 0.04486348107457161\n",
            "Epoch: 146, train loss: 0.04460553824901581\n",
            "Epoch: 147, train loss: 0.04435081034898758\n",
            "Epoch: 148, train loss: 0.04409923776984215\n",
            "Epoch: 149, train loss: 0.043850768357515335\n",
            "Epoch: 150, train loss: 0.043605316430330276\n",
            "Epoch: 151, train loss: 0.04336283355951309\n",
            "Epoch: 152, train loss: 0.04312329366803169\n",
            "Epoch: 153, train loss: 0.042886603623628616\n",
            "Epoch: 154, train loss: 0.04265272989869118\n",
            "Epoch: 155, train loss: 0.0424216166138649\n",
            "Epoch: 156, train loss: 0.042193200439214706\n",
            "Epoch: 157, train loss: 0.04196736961603165\n",
            "Epoch: 158, train loss: 0.04174399375915527\n",
            "Epoch: 159, train loss: 0.04152316972613335\n",
            "Epoch: 160, train loss: 0.04130489006638527\n",
            "Epoch: 161, train loss: 0.04108906537294388\n",
            "Epoch: 162, train loss: 0.04087570309638977\n",
            "Epoch: 163, train loss: 0.04066471382975578\n",
            "Epoch: 164, train loss: 0.04045609384775162\n",
            "Epoch: 165, train loss: 0.040249768644571304\n",
            "Epoch: 166, train loss: 0.04004571959376335\n",
            "Epoch: 167, train loss: 0.039843909442424774\n",
            "Epoch: 168, train loss: 0.039644308388233185\n",
            "Epoch: 169, train loss: 0.039446841925382614\n",
            "Epoch: 170, train loss: 0.039251506328582764\n",
            "Epoch: 171, train loss: 0.039058271795511246\n",
            "Epoch: 172, train loss: 0.03886706382036209\n",
            "Epoch: 173, train loss: 0.0386778861284256\n",
            "Epoch: 174, train loss: 0.03849069029092789\n",
            "Epoch: 175, train loss: 0.038305461406707764\n",
            "Epoch: 176, train loss: 0.03812213987112045\n",
            "Epoch: 177, train loss: 0.03794069588184357\n",
            "Epoch: 178, train loss: 0.037761129438877106\n",
            "Epoch: 179, train loss: 0.03758338838815689\n",
            "Epoch: 180, train loss: 0.037407439202070236\n",
            "Epoch: 181, train loss: 0.03723328188061714\n",
            "Epoch: 182, train loss: 0.03706086426973343\n",
            "Epoch: 183, train loss: 0.03689015656709671\n",
            "Epoch: 184, train loss: 0.0367211177945137\n",
            "Epoch: 185, train loss: 0.03655378147959709\n",
            "Epoch: 186, train loss: 0.036388058215379715\n",
            "Epoch: 187, train loss: 0.03622395917773247\n",
            "Epoch: 188, train loss: 0.036061447113752365\n",
            "Epoch: 189, train loss: 0.03590049594640732\n",
            "Epoch: 190, train loss: 0.03574109822511673\n",
            "Epoch: 191, train loss: 0.035583216696977615\n",
            "Epoch: 192, train loss: 0.035426829010248184\n",
            "Epoch: 193, train loss: 0.03527189418673515\n",
            "Epoch: 194, train loss: 0.035118430852890015\n",
            "Epoch: 195, train loss: 0.034966401755809784\n",
            "Epoch: 196, train loss: 0.03481578081846237\n",
            "Epoch: 197, train loss: 0.03466654196381569\n",
            "Epoch: 198, train loss: 0.034518688917160034\n",
            "Epoch: 199, train loss: 0.03437216207385063\n",
            "Epoch: 200, train loss: 0.03422698751091957\n",
            "Epoch: 201, train loss: 0.03408312052488327\n",
            "Epoch: 202, train loss: 0.03394053503870964\n",
            "Epoch: 203, train loss: 0.03379923477768898\n",
            "Epoch: 204, train loss: 0.0336591936647892\n",
            "Epoch: 205, train loss: 0.03352038562297821\n",
            "Epoch: 206, train loss: 0.03338281065225601\n",
            "Epoch: 207, train loss: 0.03324643895030022\n",
            "Epoch: 208, train loss: 0.03311123698949814\n",
            "Epoch: 209, train loss: 0.03297724574804306\n",
            "Epoch: 210, train loss: 0.032844364643096924\n",
            "Epoch: 211, train loss: 0.0327126570045948\n",
            "Epoch: 212, train loss: 0.03258207440376282\n",
            "Epoch: 213, train loss: 0.03245258703827858\n",
            "Epoch: 214, train loss: 0.032324232161045074\n",
            "Epoch: 215, train loss: 0.03219691663980484\n",
            "Epoch: 216, train loss: 0.03207068517804146\n",
            "Epoch: 217, train loss: 0.03194551542401314\n",
            "Epoch: 218, train loss: 0.03182137757539749\n",
            "Epoch: 219, train loss: 0.03169826790690422\n",
            "Epoch: 220, train loss: 0.031576164066791534\n",
            "Epoch: 221, train loss: 0.03145507350564003\n",
            "Epoch: 222, train loss: 0.031334973871707916\n",
            "Epoch: 223, train loss: 0.03121582232415676\n",
            "Epoch: 224, train loss: 0.031097659841179848\n",
            "Epoch: 225, train loss: 0.030980432406067848\n",
            "Epoch: 226, train loss: 0.03086414560675621\n",
            "Epoch: 227, train loss: 0.030748790130019188\n",
            "Epoch: 228, train loss: 0.030634354799985886\n",
            "Epoch: 229, train loss: 0.030520811676979065\n",
            "Epoch: 230, train loss: 0.030408158898353577\n",
            "Epoch: 231, train loss: 0.03029640018939972\n",
            "Epoch: 232, train loss: 0.030185500159859657\n",
            "Epoch: 233, train loss: 0.03007545694708824\n",
            "Epoch: 234, train loss: 0.029966270551085472\n",
            "Epoch: 235, train loss: 0.029857929795980453\n",
            "Epoch: 236, train loss: 0.029750416055321693\n",
            "Epoch: 237, train loss: 0.029643720015883446\n",
            "Epoch: 238, train loss: 0.029537826776504517\n",
            "Epoch: 239, train loss: 0.02943275310099125\n",
            "Epoch: 240, train loss: 0.029328446835279465\n",
            "Epoch: 241, train loss: 0.02922494150698185\n",
            "Epoch: 242, train loss: 0.029122192412614822\n",
            "Epoch: 243, train loss: 0.029020216315984726\n",
            "Epoch: 244, train loss: 0.028918961063027382\n",
            "Epoch: 245, train loss: 0.028818491846323013\n",
            "Epoch: 246, train loss: 0.028718749061226845\n",
            "Epoch: 247, train loss: 0.028619732707738876\n",
            "Epoch: 248, train loss: 0.028521433472633362\n",
            "Epoch: 249, train loss: 0.028423842042684555\n",
            "Epoch: 250, train loss: 0.02832696959376335\n",
            "Epoch: 251, train loss: 0.028230775147676468\n",
            "Epoch: 252, train loss: 0.028135281056165695\n",
            "Epoch: 253, train loss: 0.02804046869277954\n",
            "Epoch: 254, train loss: 0.027946319431066513\n",
            "Epoch: 255, train loss: 0.02785283699631691\n",
            "Epoch: 256, train loss: 0.027760004624724388\n",
            "Epoch: 257, train loss: 0.027667853981256485\n",
            "Epoch: 258, train loss: 0.02757631614804268\n",
            "Epoch: 259, train loss: 0.027485420927405357\n",
            "Epoch: 260, train loss: 0.027395162731409073\n",
            "Epoch: 261, train loss: 0.027305522933602333\n",
            "Epoch: 262, train loss: 0.02721649967133999\n",
            "Epoch: 263, train loss: 0.027128081768751144\n",
            "Epoch: 264, train loss: 0.027040263637900352\n",
            "Epoch: 265, train loss: 0.026953060179948807\n",
            "Epoch: 266, train loss: 0.02686643972992897\n",
            "Epoch: 267, train loss: 0.0267803855240345\n",
            "Epoch: 268, train loss: 0.026694918051362038\n",
            "Epoch: 269, train loss: 0.026610031723976135\n",
            "Epoch: 270, train loss: 0.026525700464844704\n",
            "Epoch: 271, train loss: 0.02644193172454834\n",
            "Epoch: 272, train loss: 0.026358727365732193\n",
            "Epoch: 273, train loss: 0.026276055723428726\n",
            "Epoch: 274, train loss: 0.026193935424089432\n",
            "Epoch: 275, train loss: 0.02611233852803707\n",
            "Epoch: 276, train loss: 0.026031291112303734\n",
            "Epoch: 277, train loss: 0.025950759649276733\n",
            "Epoch: 278, train loss: 0.025870751589536667\n",
            "Epoch: 279, train loss: 0.02579125389456749\n",
            "Epoch: 280, train loss: 0.02571227215230465\n",
            "Epoch: 281, train loss: 0.0256338007748127\n",
            "Epoch: 282, train loss: 0.025555824860930443\n",
            "Epoch: 283, train loss: 0.025478333234786987\n",
            "Epoch: 284, train loss: 0.025401342660188675\n",
            "Epoch: 285, train loss: 0.025324830785393715\n",
            "Epoch: 286, train loss: 0.025248810648918152\n",
            "Epoch: 287, train loss: 0.025173252448439598\n",
            "Epoch: 288, train loss: 0.025098178535699844\n",
            "Epoch: 289, train loss: 0.025023559108376503\n",
            "Epoch: 290, train loss: 0.02494940347969532\n",
            "Epoch: 291, train loss: 0.024875717237591743\n",
            "Epoch: 292, train loss: 0.024802464991807938\n",
            "Epoch: 293, train loss: 0.024729672819375992\n",
            "Epoch: 294, train loss: 0.024657320231199265\n",
            "Epoch: 295, train loss: 0.024585405364632607\n",
            "Epoch: 296, train loss: 0.024513939395546913\n",
            "Epoch: 297, train loss: 0.024442898109555244\n",
            "Epoch: 298, train loss: 0.024372275918722153\n",
            "Epoch: 299, train loss: 0.024302074685692787\n",
            "Epoch: 300, train loss: 0.024232318624854088\n",
            "Epoch: 301, train loss: 0.024162959307432175\n",
            "Epoch: 302, train loss: 0.024094022810459137\n",
            "Epoch: 303, train loss: 0.024025481194257736\n",
            "Epoch: 304, train loss: 0.023957354947924614\n",
            "Epoch: 305, train loss: 0.023889627307653427\n",
            "Epoch: 306, train loss: 0.023822281509637833\n",
            "Epoch: 307, train loss: 0.023755352944135666\n",
            "Epoch: 308, train loss: 0.023688804358243942\n",
            "Epoch: 309, train loss: 0.023622630164027214\n",
            "Epoch: 310, train loss: 0.023556847125291824\n",
            "Epoch: 311, train loss: 0.023491453379392624\n",
            "Epoch: 312, train loss: 0.023426417261362076\n",
            "Epoch: 313, train loss: 0.023361755535006523\n",
            "Epoch: 314, train loss: 0.023297477513551712\n",
            "Epoch: 315, train loss: 0.023233572021126747\n",
            "Epoch: 316, train loss: 0.023169992491602898\n",
            "Epoch: 317, train loss: 0.02310677245259285\n",
            "Epoch: 318, train loss: 0.023043924942612648\n",
            "Epoch: 319, train loss: 0.022981412708759308\n",
            "Epoch: 320, train loss: 0.02291925996541977\n",
            "Epoch: 321, train loss: 0.022857431322336197\n",
            "Epoch: 322, train loss: 0.02279597334563732\n",
            "Epoch: 323, train loss: 0.022734835743904114\n",
            "Epoch: 324, train loss: 0.022674057632684708\n",
            "Epoch: 325, train loss: 0.022613609209656715\n",
            "Epoch: 326, train loss: 0.02255347929894924\n",
            "Epoch: 327, train loss: 0.022493699565529823\n",
            "Epoch: 328, train loss: 0.02243422530591488\n",
            "Epoch: 329, train loss: 0.02237507700920105\n",
            "Epoch: 330, train loss: 0.02231626585125923\n",
            "Epoch: 331, train loss: 0.022257765755057335\n",
            "Epoch: 332, train loss: 0.022199586033821106\n",
            "Epoch: 333, train loss: 0.022141698747873306\n",
            "Epoch: 334, train loss: 0.02208412066102028\n",
            "Epoch: 335, train loss: 0.02202688902616501\n",
            "Epoch: 336, train loss: 0.02196994423866272\n",
            "Epoch: 337, train loss: 0.02191329561173916\n",
            "Epoch: 338, train loss: 0.021856937557458878\n",
            "Epoch: 339, train loss: 0.02180088683962822\n",
            "Epoch: 340, train loss: 0.02174513414502144\n",
            "Epoch: 341, train loss: 0.021689677610993385\n",
            "Epoch: 342, train loss: 0.02163449302315712\n",
            "Epoch: 343, train loss: 0.02157963067293167\n",
            "Epoch: 344, train loss: 0.021525021642446518\n",
            "Epoch: 345, train loss: 0.02147071622312069\n",
            "Epoch: 346, train loss: 0.02141667529940605\n",
            "Epoch: 347, train loss: 0.021362926810979843\n",
            "Epoch: 348, train loss: 0.021309448406100273\n",
            "Epoch: 349, train loss: 0.02125624008476734\n",
            "Epoch: 350, train loss: 0.021203305572271347\n",
            "Epoch: 351, train loss: 0.021150648593902588\n",
            "Epoch: 352, train loss: 0.021098246797919273\n",
            "Epoch: 353, train loss: 0.02104613184928894\n",
            "Epoch: 354, train loss: 0.020994262769818306\n",
            "Epoch: 355, train loss: 0.020942656323313713\n",
            "Epoch: 356, train loss: 0.020891321823000908\n",
            "Epoch: 357, train loss: 0.0208402331918478\n",
            "Epoch: 358, train loss: 0.02078939601778984\n",
            "Epoch: 359, train loss: 0.02073882706463337\n",
            "Epoch: 360, train loss: 0.020688500255346298\n",
            "Epoch: 361, train loss: 0.020638413727283478\n",
            "Epoch: 362, train loss: 0.020588595420122147\n",
            "Epoch: 363, train loss: 0.02053900621831417\n",
            "Epoch: 364, train loss: 0.020489666610956192\n",
            "Epoch: 365, train loss: 0.02044057473540306\n",
            "Epoch: 366, train loss: 0.020391710102558136\n",
            "Epoch: 367, train loss: 0.02034308761358261\n",
            "Epoch: 368, train loss: 0.020294707268476486\n",
            "Epoch: 369, train loss: 0.02024655230343342\n",
            "Epoch: 370, train loss: 0.020198646932840347\n",
            "Epoch: 371, train loss: 0.020150957629084587\n",
            "Epoch: 372, train loss: 0.02010349929332733\n",
            "Epoch: 373, train loss: 0.020056268200278282\n",
            "Epoch: 374, train loss: 0.020009269937872887\n",
            "Epoch: 375, train loss: 0.0199624951928854\n",
            "Epoch: 376, train loss: 0.01991594396531582\n",
            "Epoch: 377, train loss: 0.019869599491357803\n",
            "Epoch: 378, train loss: 0.01982349157333374\n",
            "Epoch: 379, train loss: 0.01977759227156639\n",
            "Epoch: 380, train loss: 0.0197319183498621\n",
            "Epoch: 381, train loss: 0.019686441868543625\n",
            "Epoch: 382, train loss: 0.01964118890464306\n",
            "Epoch: 383, train loss: 0.019596150144934654\n",
            "Epoch: 384, train loss: 0.019551310688257217\n",
            "Epoch: 385, train loss: 0.019506704062223434\n",
            "Epoch: 386, train loss: 0.01946227438747883\n",
            "Epoch: 387, train loss: 0.01941806636750698\n",
            "Epoch: 388, train loss: 0.01937405951321125\n",
            "Epoch: 389, train loss: 0.019330250099301338\n",
            "Epoch: 390, train loss: 0.019286639988422394\n",
            "Epoch: 391, train loss: 0.019243229180574417\n",
            "Epoch: 392, train loss: 0.0192000363022089\n",
            "Epoch: 393, train loss: 0.019157009199261665\n",
            "Epoch: 394, train loss: 0.019114194437861443\n",
            "Epoch: 395, train loss: 0.019071590155363083\n",
            "Epoch: 396, train loss: 0.019029153510928154\n",
            "Epoch: 397, train loss: 0.018986901268363\n",
            "Epoch: 398, train loss: 0.018944857642054558\n",
            "Epoch: 399, train loss: 0.018903011456131935\n",
            "Epoch: 400, train loss: 0.018861334770917892\n",
            "Epoch: 401, train loss: 0.01881984807550907\n",
            "Epoch: 402, train loss: 0.018778543919324875\n",
            "Epoch: 403, train loss: 0.0187374260276556\n",
            "Epoch: 404, train loss: 0.018696488812565804\n",
            "Epoch: 405, train loss: 0.018655728548765182\n",
            "Epoch: 406, train loss: 0.018615148961544037\n",
            "Epoch: 407, train loss: 0.01857476681470871\n",
            "Epoch: 408, train loss: 0.018534531816840172\n",
            "Epoch: 409, train loss: 0.0184944961220026\n",
            "Epoch: 410, train loss: 0.018454618752002716\n",
            "Epoch: 411, train loss: 0.018414922058582306\n",
            "Epoch: 412, train loss: 0.018375400453805923\n",
            "Epoch: 413, train loss: 0.01833604834973812\n",
            "Epoch: 414, train loss: 0.01829686388373375\n",
            "Epoch: 415, train loss: 0.01825784705579281\n",
            "Epoch: 416, train loss: 0.0182189978659153\n",
            "Epoch: 417, train loss: 0.018180325627326965\n",
            "Epoch: 418, train loss: 0.018141809850931168\n",
            "Epoch: 419, train loss: 0.01810345984995365\n",
            "Epoch: 420, train loss: 0.018065260723233223\n",
            "Epoch: 421, train loss: 0.018027249723672867\n",
            "Epoch: 422, train loss: 0.017989391461014748\n",
            "Epoch: 423, train loss: 0.017951685935258865\n",
            "Epoch: 424, train loss: 0.017914138734340668\n",
            "Epoch: 425, train loss: 0.017876744270324707\n",
            "Epoch: 426, train loss: 0.017839519307017326\n",
            "Epoch: 427, train loss: 0.01780245080590248\n",
            "Epoch: 428, train loss: 0.017765527591109276\n",
            "Epoch: 429, train loss: 0.017728764563798904\n",
            "Epoch: 430, train loss: 0.017692143097519875\n",
            "Epoch: 431, train loss: 0.017655687406659126\n",
            "Epoch: 432, train loss: 0.017619391903281212\n",
            "Epoch: 433, train loss: 0.017583247274160385\n",
            "Epoch: 434, train loss: 0.01754721999168396\n",
            "Epoch: 435, train loss: 0.017511358484625816\n",
            "Epoch: 436, train loss: 0.01747564971446991\n",
            "Epoch: 437, train loss: 0.017440086230635643\n",
            "Epoch: 438, train loss: 0.01740465685725212\n",
            "Epoch: 439, train loss: 0.01736939325928688\n",
            "Epoch: 440, train loss: 0.01733424887061119\n",
            "Epoch: 441, train loss: 0.017299270257353783\n",
            "Epoch: 442, train loss: 0.017264414578676224\n",
            "Epoch: 443, train loss: 0.017229700461030006\n",
            "Epoch: 444, train loss: 0.017195140942931175\n",
            "Epoch: 445, train loss: 0.01716071180999279\n",
            "Epoch: 446, train loss: 0.01712642051279545\n",
            "Epoch: 447, train loss: 0.017092248424887657\n",
            "Epoch: 448, train loss: 0.017058243975043297\n",
            "Epoch: 449, train loss: 0.01702435314655304\n",
            "Epoch: 450, train loss: 0.016990620642900467\n",
            "Epoch: 451, train loss: 0.016956999897956848\n",
            "Epoch: 452, train loss: 0.01692351885139942\n",
            "Epoch: 453, train loss: 0.016890164464712143\n",
            "Epoch: 454, train loss: 0.016856949776411057\n",
            "Epoch: 455, train loss: 0.016823867335915565\n",
            "Epoch: 456, train loss: 0.016790922731161118\n",
            "Epoch: 457, train loss: 0.016758102923631668\n",
            "Epoch: 458, train loss: 0.016725389286875725\n",
            "Epoch: 459, train loss: 0.016692811623215675\n",
            "Epoch: 460, train loss: 0.01666037179529667\n",
            "Epoch: 461, train loss: 0.01662805862724781\n",
            "Epoch: 462, train loss: 0.01659586653113365\n",
            "Epoch: 463, train loss: 0.01656380668282509\n",
            "Epoch: 464, train loss: 0.01653185673058033\n",
            "Epoch: 465, train loss: 0.016500037163496017\n",
            "Epoch: 466, train loss: 0.016468331217765808\n",
            "Epoch: 467, train loss: 0.016436772421002388\n",
            "Epoch: 468, train loss: 0.016405295580625534\n",
            "Epoch: 469, train loss: 0.01637386716902256\n",
            "Epoch: 470, train loss: 0.01634255237877369\n",
            "Epoch: 471, train loss: 0.016311366111040115\n",
            "Epoch: 472, train loss: 0.01628030464053154\n",
            "Epoch: 473, train loss: 0.01624935492873192\n",
            "Epoch: 474, train loss: 0.0162185151129961\n",
            "Epoch: 475, train loss: 0.016187801957130432\n",
            "Epoch: 476, train loss: 0.016157198697328568\n",
            "Epoch: 477, train loss: 0.016126705333590508\n",
            "Epoch: 478, train loss: 0.01609634794294834\n",
            "Epoch: 479, train loss: 0.016066094860434532\n",
            "Epoch: 480, train loss: 0.01603594981133938\n",
            "Epoch: 481, train loss: 0.01600591279566288\n",
            "Epoch: 482, train loss: 0.01597599871456623\n",
            "Epoch: 483, train loss: 0.015946196392178535\n",
            "Epoch: 484, train loss: 0.01591649278998375\n",
            "Epoch: 485, train loss: 0.015886906534433365\n",
            "Epoch: 486, train loss: 0.015857430174946785\n",
            "Epoch: 487, train loss: 0.01582806557416916\n",
            "Epoch: 488, train loss: 0.01579880528151989\n",
            "Epoch: 489, train loss: 0.015769653022289276\n",
            "Epoch: 490, train loss: 0.015740612521767616\n",
            "Epoch: 491, train loss: 0.015711668878793716\n",
            "Epoch: 492, train loss: 0.01568284071981907\n",
            "Epoch: 493, train loss: 0.015654120594263077\n",
            "Epoch: 494, train loss: 0.0156254880130291\n",
            "Epoch: 495, train loss: 0.015596973709762096\n",
            "Epoch: 496, train loss: 0.015568557195365429\n",
            "Epoch: 497, train loss: 0.015540236607193947\n",
            "Epoch: 498, train loss: 0.01551203615963459\n",
            "Epoch: 499, train loss: 0.015483936294913292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o2MzFpgeXhW",
        "outputId": "a2f20eaf-eaf0-4409-9389-30670a6c210a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test Mode again\n",
        "\n",
        "model.eval()\n",
        "y_pred = model(x_test)\n",
        "after_train = criterion(y_pred.squeeze(), y_test)\n",
        "print('Test loss after Training:', after_train.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss after Training: 1.8983100652694702\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}